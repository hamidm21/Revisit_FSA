{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Aware Embeddings\n",
    "1. loading textual and price datasets\n",
    "2. labeling the price dataset and merging it with the textual dataset\n",
    "3. evaluating the base model on the labels\n",
    "4. finetuning the embedding model on labelings to make it impact-aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW, TrainerCallback\n",
    "from tqdm.notebook import tqdm  # Use notebook version of tqdm for better compatibility with Jupyter\n",
    "from datetime import timedelta\n",
    "import plotly.graph_objects as go\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset as torchDS\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from functools import partial\n",
    "from scipy.special import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal imports\n",
    "import sys\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_working_directory))\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting mlflow parameters\n",
    "mlflow.set_tracking_uri(uri=\"https://mlflow.finbright.org/\")\n",
    "mlflow.set_experiment(\"base_tbl_1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading textual and price datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading embedded textual dataset which should contain (date, text, embeddings)\n",
    "# TODO: generate the dataset if it doesn't exist\n",
    "text_df = pd.read_csv(\"../raw/combined_tweets_2020_labeled.csv\", usecols=[\"date\", \"text_split\"])\n",
    "text_df.rename(columns={\"text_split\": \"text\"}, inplace=True)\n",
    "text_df.set_index('date', inplace=True)\n",
    "text_df.index = pd.to_datetime(text_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading price data\n",
    "price_df = pd.read_csv(\"../raw/86400-2020.csv\", usecols=[\"timestamp\", \"close\", \"open\", \"high\", \"low\", \"volume\"])\n",
    "price_df.set_index('timestamp', inplace=True)\n",
    "price_df.index = pd.to_datetime(price_df.index, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labeling the price dataset and merging with textual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = triple_barrier_labeling(price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = tbl.iloc[0:40]\n",
    "# Create the candlestick chart\n",
    "fig = go.Figure(data=[go.Candlestick(x=tbl.index,\n",
    "                                     open=plot_df['open'], \n",
    "                                     high=plot_df['high'],\n",
    "                                     low=plot_df['low'], \n",
    "                                     close=plot_df['close'],\n",
    "                                     hovertext=['Open: {}<br>High: {}<br>Low: {}<br>Close: {}'.format(o, h, l, c) for o, h, l, c in zip(plot_df['open'], plot_df['high'], plot_df['low'], plot_df['close'])],\n",
    "                                     hoverinfo='text')])\n",
    "\n",
    "# Add the labels to the plot as a scatter plot overlay\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['close'], mode='markers', \n",
    "                         marker=dict(color=plot_df['label'].map({2: 'green', 0: 'red', 1: 'blue'}))))\n",
    "\n",
    "# Add the upper and lower barriers to the plot\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['upper_barrier'], name='Upper Barrier', line=dict(color='orange')))\n",
    "fig.add_trace(go.Scatter(x=plot_df.index, y=plot_df['lower_barrier'], name='Lower Barrier', line=dict(color='orange')))\n",
    "\n",
    "fig.show()\n",
    "del(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = text_df.merge(tbl[['label']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(labeled_df[['text', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling the dataset for a more unbiased mix\n",
    "shuffled_dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the base model on the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = softmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text field in the dataset\n",
    "def tokenize_function(tokenizer, examples):\n",
    "    # Tokenize the text and return only the necessary fields\n",
    "    encoded = tokenizer(examples[\"text\"], padding='max_length', max_length=512)\n",
    "    return {\"input_ids\": encoded[\"input_ids\"], \"attention_mask\": encoded[\"attention_mask\"], \"label\": examples[\"label\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the dataset text to be used in train and test loops\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ElKulako/cryptobert\")\n",
    "partial_tokenize_function = partial(tokenize_function, tokenizer)\n",
    "tokenized_datasets = shuffled_dataset.map(partial_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the dataset for evaluation\n",
    "subset = tokenized_datasets.select(range(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the non-fine-tuned model\n",
    "non_fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3)\n",
    "non_fine_tuned_training_args = TrainingArguments(\n",
    "    output_dir=\"../artifact\"\n",
    ")\n",
    "non_fine_tuned_trainer = Trainer(\n",
    "    model=non_fine_tuned_model,         # the non-fine-tuned model\n",
    "    args=non_fine_tuned_training_args,  # training arguments, defined above\n",
    "    eval_dataset=subset,                # test dataset\n",
    "    compute_metrics=compute_metrics,    # the compute_metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MLflow run\n",
    "with mlflow.start_run(description=\"tbl labeling, 500 shuffled dataset\", log_system_metrics=False):\n",
    "    # Train and evaluate the model\n",
    "    # non_fine_tuned_trainer.train()\n",
    "    non_fine_tuned_eval_result = non_fine_tuned_trainer.evaluate()\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(non_fine_tuned_model, \"model\")\n",
    "\n",
    "    # Log parameters\n",
    "    log_param(\"num_labels\", 3)\n",
    "\n",
    "    # Log metrics\n",
    "    for key, value in non_fine_tuned_eval_result.items():\n",
    "        log_metric(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetuning the embedding model on labelings to make it impact-aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torchDS):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could be used inside the huggingface trainer api\n",
    "class MlflowLoggingCallback(TrainerCallback):\n",
    "    \"A callback that logs metrics to MLflow\"\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, optimizer=None, **kwargs):\n",
    "        # Evaluate each step\n",
    "        step = state.global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_log_metrics(labels, preds, probs, step):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probs, multi_class='ovr')  # Use probabilities here\n",
    "\n",
    "    # Log the metrics with MLflow\n",
    "    mlflow.log_metric(\"accuracy\", acc, step=step)\n",
    "    mlflow.log_metric(\"f1\", f1, step=step)\n",
    "    mlflow.log_metric(\"precision\", precision, step=step)\n",
    "    mlflow.log_metric(\"recall\", recall, step=step)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = tokenized_datasets.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_split = subset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(subset_split['test'])\n",
    "test_dataset = TextDataset(subset_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Move the model to the device\n",
    "fine_tuned_model.to(device)\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(fine_tuned_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run(description=\"tbl labeling, 200 items, 5 epochs, full metrics, shuffled dataset, fine tuned\"):\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):  # Number of epochs\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []  # For storing probabilities\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = fine_tuned_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Log metrics with MLflow\n",
    "            mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "            \n",
    "            # Store labels, predictions and probabilities for metrics calculation\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(preds.detach().cpu().numpy())  # Store probabilities\n",
    "            class_preds = torch.argmax(preds, dim=-1)\n",
    "            all_preds.append(class_preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "        # Calculate and log metrics after each epoch\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_probs = np.concatenate(all_probs)  # Concatenate probabilities\n",
    "        calculate_and_log_metrics(all_labels, all_preds, all_probs, step=epoch)\n",
    "\n",
    "    # Evaluation loop\n",
    "    eval_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []  # For storing probabilities\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = fine_tuned_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            eval_loss += outputs.loss.item()\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            # Get the predicted probabilities from the model's outputs\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(preds.cpu().numpy())  # Store probabilities\n",
    "            # Convert the probabilities to class labels\n",
    "            class_preds = torch.argmax(preds, dim=-1)\n",
    "            all_preds.append(class_preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs)  # Concatenate probabilities\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  # Use probabilities here\n",
    "    \n",
    "    # Log the evaluation loss and metrics with MLflow\n",
    "    mlflow.log_metric(\"eval_loss\", eval_loss / len(test_dataloader), step=epoch)\n",
    "    mlflow.log_metric(\"accuracy\", acc, step=epoch)\n",
    "    mlflow.log_metric(\"f1\", f1, step=epoch)\n",
    "    mlflow.log_metric(\"precision\", precision, step=epoch)\n",
    "    mlflow.log_metric(\"recall\", recall, step=epoch)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc, step=epoch)\n",
    "    \n",
    "    # Save and log the model with MLflow\n",
    "    # torch.save(fine_tuned_model.state_dict(), \"model.pth\")\n",
    "    mlflow.pytorch.log_model(fine_tuned_model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MLflow run\n",
    "with mlflow.start_run(description=\"tbl labeling, 500 shuffled dataset, fine tuned\"):\n",
    "    # Train and evaluate the model\n",
    "    training_results = trainer.train()\n",
    "    fine_tuned_eval_result = trainer.evaluate()\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(fine_tuned_model, \"model\")\n",
    "\n",
    "    # Log parameters\n",
    "    log_param(\"num_labels\", 3)\n",
    "\n",
    "    # Log metrics\n",
    "    for key, value in fine_tuned_eval_result.items():\n",
    "        log_metric(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface trainer api for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='../artifact',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=5,  # batch size per device during training\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='../log',            # directory for storing logs\n",
    "# )\n",
    "\n",
    "# # Create the Trainer and train\n",
    "# trainer = Trainer(\n",
    "#     model=fine_tuned_model,                       # the instantiated ðŸ¤— Transformers model to be trained\n",
    "#     args=training_args,                # training arguments, defined above\n",
    "#     train_dataset=subset_split['train'],    # train dataset\n",
    "#     eval_dataset=subset_split['test'],      # test dataset\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[MlflowLoggingCallback],  # add the callback\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
