{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal and market aware embeddings\n",
    "1. load price and textual datasets\n",
    "2. prefix tweets with their time and price context\n",
    "3. fine-tune the model on text with and without the context\n",
    "4. comparing the base model with the fine-tuned masking model on the new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset as torchDS\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm.notebook import tqdm  # Use notebook version of tqdm for better compatibility with Jupyter\n",
    "from torch.optim import AdamW\n",
    "import neptune\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "sys.path.append(os.path.dirname(current_working_directory))\n",
    "from src.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load price and textual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\"../raw/combined_tweets_2020_labeled.csv\", usecols=[\"date\", \"text_split\"])\n",
    "text_df.rename(columns={\"text_split\": \"text\"}, inplace=True)\n",
    "text_df.set_index('date', inplace=True)\n",
    "text_df.index = pd.to_datetime(text_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = pd.read_csv(\"../raw/daily-2020.csv\", usecols=[\"timestamp\", \"close\", \"open\", \"high\", \"low\", \"volume\"])\n",
    "price_df.set_index('timestamp', inplace=True)\n",
    "price_df.index = pd.to_datetime(price_df.index, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64310, 1) (366, 5)\n"
     ]
    }
   ],
   "source": [
    "print(text_df.shape, price_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prefix tweets with their time and price context\n",
    "how do we define temporal and price context in this scenario\n",
    "- temporal context: as the month and year of each tweet (e.g. Mar, 2020)\n",
    "- price context: moving average of the price, trend, percent change? No, tag the current trend based on triple barrier labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_string(df):\n",
    "    \"\"\"\n",
    "    Extract time string from date column to be used in the tweet\n",
    "    \"\"\"\n",
    "    df['time'] = df.index.to_series().dt.strftime('%d,%b,%Y')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_text_column(df, time_col, trend_col, text_col):\n",
    "    \"\"\"\n",
    "    Prefix a text column with temporal and market context.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    time_col (str): The name of the time column.\n",
    "    trend_col (str): The name of the trend column.\n",
    "    text_col (str): The name of the text column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the prefixed text column.\n",
    "    \"\"\"\n",
    "    # Create a new column by combining the time, trend, and text columns\n",
    "    df[\"context_aware\"] = \"time: \" + df[time_col].astype(str) + \" trend: \" + df[trend_col].astype(str) + \" text: \" + df[text_col]\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_equal_samples(df, n_samples):\n",
    "    \"\"\"\n",
    "    Select equal numbers of tweets from each day in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    n_samples (int): The number of samples to select from each day.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the selected samples.\n",
    "    \"\"\"\n",
    "    # Get the unique dates\n",
    "    unique_dates = df.index.unique()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the selected samples\n",
    "    selected_samples = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each unique date\n",
    "    for date in unique_dates:\n",
    "        # Select n_samples from the current date\n",
    "        samples = df.loc[date].sample(n_samples, replace=True)\n",
    "\n",
    "        # Append the samples to the selected_samples DataFrame\n",
    "        selected_samples = pd.concat([selected_samples, samples])\n",
    "\n",
    "    # Return the selected_samples DataFrame\n",
    "    return selected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = triple_barrier_labeling(price_df)\n",
    "price_df[\"text_label\"] = price_df.label.map({0: 'bearish', 1: 'neutral', 2: 'bullish'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df[\"label\"] = price_df.label.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>upper_barrier</th>\n",
       "      <th>lower_barrier</th>\n",
       "      <th>vertical_barrier</th>\n",
       "      <th>label</th>\n",
       "      <th>text_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>7341.60</td>\n",
       "      <td>7400.00</td>\n",
       "      <td>7269.21</td>\n",
       "      <td>7350.71</td>\n",
       "      <td>92586.033</td>\n",
       "      <td>7532.141244</td>\n",
       "      <td>7169.278756</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>7350.54</td>\n",
       "      <td>7495.00</td>\n",
       "      <td>7303.00</td>\n",
       "      <td>7354.36</td>\n",
       "      <td>117765.972</td>\n",
       "      <td>7547.877907</td>\n",
       "      <td>7160.842093</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>7354.36</td>\n",
       "      <td>7808.65</td>\n",
       "      <td>7345.00</td>\n",
       "      <td>7757.39</td>\n",
       "      <td>168150.317</td>\n",
       "      <td>7961.689059</td>\n",
       "      <td>7553.090941</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>7757.74</td>\n",
       "      <td>8215.33</td>\n",
       "      <td>7733.00</td>\n",
       "      <td>8152.49</td>\n",
       "      <td>280809.162</td>\n",
       "      <td>8535.874250</td>\n",
       "      <td>7769.105750</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>8150.90</td>\n",
       "      <td>8468.42</td>\n",
       "      <td>7870.11</td>\n",
       "      <td>8059.84</td>\n",
       "      <td>321225.114</td>\n",
       "      <td>8419.621270</td>\n",
       "      <td>7700.058730</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-26</th>\n",
       "      <td>24729.99</td>\n",
       "      <td>26926.00</td>\n",
       "      <td>24507.24</td>\n",
       "      <td>26508.83</td>\n",
       "      <td>367265.555</td>\n",
       "      <td>27952.234709</td>\n",
       "      <td>25065.425291</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>26508.84</td>\n",
       "      <td>28459.84</td>\n",
       "      <td>25850.00</td>\n",
       "      <td>26305.64</td>\n",
       "      <td>540264.148</td>\n",
       "      <td>27626.927027</td>\n",
       "      <td>24984.352973</td>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>26301.76</td>\n",
       "      <td>27538.82</td>\n",
       "      <td>26117.10</td>\n",
       "      <td>27102.66</td>\n",
       "      <td>267563.468</td>\n",
       "      <td>28115.311395</td>\n",
       "      <td>26090.008605</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>27101.45</td>\n",
       "      <td>27441.73</td>\n",
       "      <td>25913.01</td>\n",
       "      <td>27402.83</td>\n",
       "      <td>260759.449</td>\n",
       "      <td>27912.921908</td>\n",
       "      <td>26892.738092</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>27402.83</td>\n",
       "      <td>29063.72</td>\n",
       "      <td>27401.00</td>\n",
       "      <td>28906.99</td>\n",
       "      <td>374737.655</td>\n",
       "      <td>29995.348653</td>\n",
       "      <td>27818.631347</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close      volume  upper_barrier  \\\n",
       "timestamp                                                                       \n",
       "2020-01-04   7341.60   7400.00   7269.21   7350.71   92586.033    7532.141244   \n",
       "2020-01-05   7350.54   7495.00   7303.00   7354.36  117765.972    7547.877907   \n",
       "2020-01-06   7354.36   7808.65   7345.00   7757.39  168150.317    7961.689059   \n",
       "2020-01-07   7757.74   8215.33   7733.00   8152.49  280809.162    8535.874250   \n",
       "2020-01-08   8150.90   8468.42   7870.11   8059.84  321225.114    8419.621270   \n",
       "...              ...       ...       ...       ...         ...            ...   \n",
       "2020-12-26  24729.99  26926.00  24507.24  26508.83  367265.555   27952.234709   \n",
       "2020-12-27  26508.84  28459.84  25850.00  26305.64  540264.148   27626.927027   \n",
       "2020-12-28  26301.76  27538.82  26117.10  27102.66  267563.468   28115.311395   \n",
       "2020-12-29  27101.45  27441.73  25913.01  27402.83  260759.449   27912.921908   \n",
       "2020-12-30  27402.83  29063.72  27401.00  28906.99  374737.655   29995.348653   \n",
       "\n",
       "            lower_barrier vertical_barrier  label text_label  \n",
       "timestamp                                                     \n",
       "2020-01-04    7169.278756       2020-01-11    2.0    neutral  \n",
       "2020-01-05    7160.842093       2020-01-12    2.0    bullish  \n",
       "2020-01-06    7553.090941       2020-01-13    1.0    bullish  \n",
       "2020-01-07    7769.105750       2020-01-14    1.0    neutral  \n",
       "2020-01-08    7700.058730       2020-01-15    2.0    neutral  \n",
       "...                   ...              ...    ...        ...  \n",
       "2020-12-26   25065.425291       2021-01-02    1.0    bullish  \n",
       "2020-12-27   24984.352973       2021-01-03    0.0    neutral  \n",
       "2020-12-28   26090.008605       2021-01-04    2.0    bearish  \n",
       "2020-12-29   26892.738092       2021-01-05    1.0    bullish  \n",
       "2020-12-30   27818.631347       2021-01-06    1.0    neutral  \n",
       "\n",
       "[362 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = extract_time_string(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = text_df.merge(price_df[['label', 'text_label']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = prefix_text_column(labeled_df, 'time', 'text_label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>label</th>\n",
       "      <th>text_label</th>\n",
       "      <th>context_aware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>want new york its first publicly available yen...</td>\n",
       "      <td>30,Dec,2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time: 30,Dec,2020 trend: neutral text: want ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>next decade of sustainable crypto innovation b...</td>\n",
       "      <td>30,Dec,2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time: 30,Dec,2020 trend: neutral text: next de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>bitcoin too complete simple and earn up to tra...</td>\n",
       "      <td>30,Dec,2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time: 30,Dec,2020 trend: neutral text: bitcoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>no tie to the btc dollar ratio wonder if he wo...</td>\n",
       "      <td>30,Dec,2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time: 30,Dec,2020 trend: neutral text: no tie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>rich bitcoin cad bitcoin btc bitcoin everythin...</td>\n",
       "      <td>30,Dec,2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time: 30,Dec,2020 trend: neutral text: rich bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text         time  \\\n",
       "2020-12-30  want new york its first publicly available yen...  30,Dec,2020   \n",
       "2020-12-30  next decade of sustainable crypto innovation b...  30,Dec,2020   \n",
       "2020-12-30  bitcoin too complete simple and earn up to tra...  30,Dec,2020   \n",
       "2020-12-30  no tie to the btc dollar ratio wonder if he wo...  30,Dec,2020   \n",
       "2020-12-30  rich bitcoin cad bitcoin btc bitcoin everythin...  30,Dec,2020   \n",
       "\n",
       "            label text_label  \\\n",
       "2020-12-30    1.0    neutral   \n",
       "2020-12-30    1.0    neutral   \n",
       "2020-12-30    1.0    neutral   \n",
       "2020-12-30    1.0    neutral   \n",
       "2020-12-30    1.0    neutral   \n",
       "\n",
       "                                                context_aware  \n",
       "2020-12-30  time: 30,Dec,2020 trend: neutral text: want ne...  \n",
       "2020-12-30  time: 30,Dec,2020 trend: neutral text: next de...  \n",
       "2020-12-30  time: 30,Dec,2020 trend: neutral text: bitcoin...  \n",
       "2020-12-30  time: 30,Dec,2020 trend: neutral text: no tie ...  \n",
       "2020-12-30  time: 30,Dec,2020 trend: neutral text: rich bi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_tweets_per_day = 100\n",
    "sampled_df = select_equal_samples(labeled_df, how_many_tweets_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_label\n",
       "bullish    15500\n",
       "neutral    10600\n",
       "bearish    10100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.text_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(sampled_df[['text', 'context_aware', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tune the model on text with and without the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neptune_run(name, description, params):\n",
    "    \"\"\"\n",
    "    initializes and returns an instance of neptune run and sends the parameters\n",
    "    \"\"\"\n",
    "    run = neptune.init_run(\n",
    "    proxies={\n",
    "        \"http\": \"http://tracker:nlOv5rC7cL3q3bYR@95.216.41.71:3128\",\n",
    "        \"https\": \"http://tracker:nlOv5rC7cL3q3bYR@95.216.41.71:3128\"\n",
    "    },\n",
    "    project=\"Financial-NLP/market-aware-embedding\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2YWViODAxNC05MzNkLTRiZGMtOGI4My04M2U3MDViN2U3ODEifQ==\",\n",
    "    name=name,\n",
    "    description=description\n",
    "    )\n",
    "\n",
    "    run[\"parameters\"] = params\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torchDS):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, preds, probs):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = softmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'conf_matrix': conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text field in the dataset\n",
    "def tokenize_function(tokenizer, examples, text_col=\"text\"):\n",
    "    # Tokenize the text and return only the necessary fields\n",
    "    encoded = tokenizer(examples[text_col], padding='max_length', max_length=512)\n",
    "    return {\"input_ids\": encoded[\"input_ids\"], \"attention_mask\": encoded[\"attention_mask\"], \"label\": examples[\"label\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, epochs, learning_rate, neptune_run=None, device=None):\n",
    "    epoch_results = {}\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    # Set up the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):  # Number of epochs\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []  # For storing probabilities\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].long().to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store labels, predictions and probabilities for metrics calculation\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(preds.detach().cpu().numpy())  # Store probabilities\n",
    "            class_preds = torch.argmax(preds, dim=-1)\n",
    "            all_preds.append(class_preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            results = compute_metrics(np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_probs))\n",
    "            if neptune_run != None:\n",
    "                neptune_run[\"eval/accuracy\"].log(results[\"accuracy\"])\n",
    "                neptune_run[\"eval/precision\"].log(results[\"precision\"])\n",
    "                neptune_run[\"eval/recall\"].log(results[\"recall\"])\n",
    "                neptune_run[\"eval/f1\"].log(results[\"f1\"])\n",
    "\n",
    "        # Calculate and log metrics after each epoch\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_probs = np.concatenate(all_probs)  # Concatenate probabilities\n",
    "        results = compute_metrics(all_labels, all_preds, all_probs)\n",
    "        if neptune_run != None:\n",
    "            neptune_run[\"train/accuracy\"].log(results[\"accuracy\"])\n",
    "            neptune_run[\"train/precision\"].log(results[\"precision\"])\n",
    "            neptune_run[\"train/recall\"].log(results[\"recall\"])\n",
    "            neptune_run[\"train/f1\"].log(results[\"f1\"])\n",
    "        epoch_results[epoch] = results\n",
    "\n",
    "    # Return both the trained model and the epoch results\n",
    "    return model, epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_dataloader, neptune_run=None, device=None):\n",
    "    eval_results = {}\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []  # For storing probabilities\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].long().to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Store labels, predictions and probabilities for metrics calculation\n",
    "            preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(preds.detach().cpu().numpy())  # Store probabilities\n",
    "            class_preds = torch.argmax(preds, dim=-1)\n",
    "            all_preds.append(class_preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            results = compute_metrics(np.concatenate(all_labels), np.concatenate(all_preds), np.concatenate(all_probs))\n",
    "            if neptune_run != None:\n",
    "                neptune_run[\"eval/accuracy\"].log(results[\"accuracy\"])\n",
    "                neptune_run[\"eval/precision\"].log(results[\"precision\"])\n",
    "                neptune_run[\"eval/recall\"].log(results[\"recall\"])\n",
    "                neptune_run[\"eval/f1\"].log(results[\"f1\"])\n",
    "            \n",
    "\n",
    "        # Calculate and log metrics after each epoch\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_probs = np.concatenate(all_probs)  # Concatenate probabilities\n",
    "        results = compute_metrics(all_labels, all_preds, all_probs)\n",
    "        if neptune_run != None:\n",
    "            neptune_run[\"eval/accuracy\"].log(results[\"accuracy\"])\n",
    "            neptune_run[\"eval/precision\"].log(results[\"precision\"])\n",
    "            neptune_run[\"eval/recall\"].log(results[\"recall\"])\n",
    "            neptune_run[\"eval/f1\"].log(results[\"f1\"])\n",
    "\n",
    "        eval_results = results\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the dataset text to be used in train and test loops\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ElKulako/cryptobert\")\n",
    "partial_tokenize_function_text = partial(tokenize_function, tokenizer, text_col=\"text\")\n",
    "partial_tokenize_function_context = partial(tokenize_function, tokenizer, text_col=\"context_aware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36851dbca164092bcd1689ddee2204f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32151dc7ef1944c48675d17fa2115bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed94a7e872f4d51b2d4d4dac5f6ca45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e67916bf0a442f947027b86b2134bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "tokenized_train_text = dataset[\"train\"].map(partial_tokenize_function_text, batched=True)\n",
    "tokenized_test_text = dataset[\"test\"].map(partial_tokenize_function_text, batched=True)\n",
    "tokenized_train_context = dataset[\"train\"].map(partial_tokenize_function_context, batched=True)\n",
    "tokenized_test_context = dataset[\"test\"].map(partial_tokenize_function_context, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_text_dataset = TextDataset(tokenized_train_text)\n",
    "tokenized_test_text_dataset = TextDataset(tokenized_test_text)\n",
    "tokenized_train_context_dataset = TextDataset(tokenized_train_context)\n",
    "tokenized_test_context_dataset = TextDataset(tokenized_test_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=10\n",
    "tokenized_train_text_dataloader = DataLoader(tokenized_train_text_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tokenized_test_text_dataloader = DataLoader(tokenized_test_text_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tokenized_train_context_dataloader = DataLoader(tokenized_train_context_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tokenized_test_context_dataloader = DataLoader(tokenized_test_context_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing the base model with the fine-tuned masking model on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_batches\": tokenized_train_text_dataloader,\n",
    "    \"test_batches\": tokenized_test_text_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = init_neptune_run(name=\"base_text_model\", description=\"base model fine-tuned on textual data without temporal or market context\", params=params)\n",
    "text_trained_model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3)\n",
    "text_trained_model, _ = train_model(text_trained_model, tokenized_train_text_dataloader, epochs, learning_rate, run)\n",
    "evaluate_model(text_trained_model, tokenized_test_text_dataloader, neptune_run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = init_neptune_run(name=\"temporal_context_model\", description=\"base model fine-tuned on textual data with temporal and market context\", params=params)\n",
    "context_trained_model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3)\n",
    "context_trained_model, _, = train_model(context_trained_model, tokenized_train_context_dataloader, epochs, learning_rate, run)\n",
    "evaluate_model(context_trained_model, tokenized_test_context_dataloader, neptune_run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = init_neptune_run(name=\"base_model\", description=\"base model without fine-tuning\", params=params)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\", num_labels=3)\n",
    "evaluate_model(base_model, tokenized_test_text_dataloader, neptune_run=run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
